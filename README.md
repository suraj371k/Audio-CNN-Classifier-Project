# Audio CNN Classifier & Visualizer ğŸ§ ğŸµ

[![Model Accuracy](https://img.shields.io/badge/Model%20Accuracy-83.4%25-brightgreen.svg)]()
[![Next.js](https://img.shields.io/badge/Next.js-14-blue.svg)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-18-blue.svg)](https://reactjs.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0-orange.svg)](https://pytorch.org/)
[![Modal](https://img.shields.io/badge/Modal-Serverless-red.svg)](https://modal.com/)

**Production-ready audio-CNN classification** trained on ResNet architecture which i created from scratch, then trained the model on **ESC-50 DATASET** achieving **83.4% accuracy**. Live **feature map visualization**, **ResNet-50**, **Modal serverless**, **TensorBoard**, **Next.js 14 + React**.

---

## âœ¨ Features

- **83.4% Top-1 Accuracy** on ESC-50 (50 environmental sounds)
- **Live Feature Maps** - Watch ResNet-50 layers activate
- **Real-time Waveform** + Mel Spectrogram visualization
- **50-class Emoji Support** ğŸ¦ğŸ‘ğŸšğŸ”¨ (chirping_birds, clapping, etc.)
- **Modal A10G GPU** serverless inference (~45ms)
- **Next.js 14 App Router** + TypeScript + shadcn/ui
- **Base64 WAV** upload â†’ instant predictions
---
## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Top-1 Accuracy** | **83.4%** |
| Inference Time | **45ms** (A10G) |
| Classes | 50 |
| Input Length | 5s WAV (44.1kHz) |
| Spectrogram | 128 mel bins |

---

**Sample Output (clapping.wav):**
ğŸ‘ clapping: 81.63%
ğŸ‘£ footsteps: 18.36%
ğŸ¥« can_opening: 0.01%

---

## ğŸ—ï¸ Architecture

**ResNet-50 for Audio Spectrograms** (3-4-6-3 blocks):

`Conv1(7Ã—7,64) â†’ Layer1(3Ã—64ch) â†’ Layer2(4Ã—128ch) â†’ Layer3(6Ã—256ch) â†’ Layer4(3Ã—512ch)`
â†“
`AdaptiveAvgPool2d â†’ Dropout(0.5) â†’ FC(512â†’50) â†’ Softmax`

--- 
**Audio Pipeline:**
`WAV â†’ Mono â†’ Resample(44.1kHz) â†’ MelSpec(n_mels=128,n_fft=1024,hop=512) â†’ dB â†’ ResNet`

---

## ğŸš€ Quick Start

### Backend (Modal)
```bash
pip install -r requirements.txt
modal token set
modal volume put esc-model best_model.pth
modal deploy main.py
```

### Frontend
```bash
cd frontend
pnpm install
pnpm dev
```

Live API: https://harshit7271--audio-cnn-classifier-audioclassifier-inference.modal.run/

---
## Structure
â”œâ”€â”€ frontend/                 # Next.js 14 T3 Stack

â”‚   â”œâ”€â”€ app/                 # App Router

â”‚   â”œâ”€â”€ components/ui/       # shadcn/ui (Badge, Card, Progress)

â”‚   â””â”€â”€ lib/utils.ts        # API helpers

â”œâ”€â”€ backend/                 # Modal + PyTorch

â”‚   â”œâ”€â”€ main.py/               # FastAPI + AudioClassifier

|   â”œâ”€â”€ train.py/             

â”‚   â”œâ”€â”€ model.py/            # ResNet-50 (16 blocks)

â”‚   â”œâ”€â”€ requirements.txt/

â”‚   â””â”€â”€ best_model.pth/      # 83.4% checkpoint

â””â”€â”€ README.md

---

# Code Highligths
### Backend(main.py)
```python
# Base64 â†’ WAV â†’ MelSpec â†’ ResNet â†’ Feature Maps
audio_bytes = base64.b64decode(request.audio_data)
spectrogram = MelSpectrogram(sample_rate=44100, n_mels=128)
output, feature_maps = model(spectrogram, return_feature_maps=True)
```

## Frontend(React)
```tsx
const base64String = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));
const { predictions, visualization, waveform } = await response.json();
```
---

# Key Features
- GPU Autoscaling - Modal scales to zero

- NaN Handling - `torch.nan_to_num()` for robust inference

- Stereoâ†’Mono - `np.mean(audio_data, axis=1)`

- Waveform Downsampling - Max 8000 samples for viz

- Layer Splitting - `splitLayers()` for block visualization

- ESC-50 Emojis - 50-class mapping (ğŸ•ğŸŒ§ï¸ğŸ‘¶ğŸšª etc.)

---
# Training
```bash
Dataset: ESC-50 (2000 clips, 5s, 44.1kHz)
Splits: 5-fold CV (Fold 5 = test)
Best: Epoch 100, Val Acc: 83.4%
```
---
#  UI Components

- Top Predictions
- Input Spectrogram
- Audio Waveform
- Convolutional Layer Outputs


---
# License
MIT - Use freely!

